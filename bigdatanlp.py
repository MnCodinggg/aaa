# -*- coding: utf-8 -*-
"""BigDataNLP_6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ycpwnsxad92xQSAkk3BYdtY5FyDZsDUk
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install requests
!pip install beautifulsoup4
!pip install pdfplumber
!pip install PyPDF2

import requests
from bs4 import BeautifulSoup
import re
import pdfplumber
import os
import datetime

# ----------------------------------------------------
# pdf 파일 다운로드 후 내용 추출
# ----------------------------------------------------
def extract_text_from_pdf_url(url, pdf_path):
    # PDF 파일을 URL에서 다운로드
    #pdf_path = "downloaded_pdf_file.pdf"  # 임시 저장 경로
    response = requests.get(url)

    # PDF 파일을 로컬에 저장
    with open(pdf_path, "wb") as file:
        file.write(response.content)

    # PDF 파일을 열어서 텍스트 추출
    extracted_text = ""
    #try:
    with pdfplumber.open(pdf_path) as pdf:
        for page_number, page in enumerate(pdf.pages, start=1):
            extracted_text += f"Page {page_number}:\n"
            extracted_text += page.extract_text() + "\n\n"
    #finally:
        # PDF 파일 삭제
        #if os.path.exists(pdf_path):
        #    os.remove(pdf_path)

    return extracted_text

# ----------------------------------------------------
# pdf 다운로드 URL 찾기
# ----------------------------------------------------
def get_pdf_url(url, printflag):
    # HTTP 요청을 보내고 응답 받기
    response = requests.get(url)

    if printflag:
        print(response.text)
    # pdb.set_trace()

    # 결과를 저장할 배열
    pdf_url = ""

    # 응답의 상태 코드 확인 (200번대면 정상)
    if response.status_code == 200:
        # HTML 파싱
        soup = BeautifulSoup(response.text, 'html.parser')
        # print(soup.text)

        # a 태그의 텍스트가 "주간기술동향"을 포함한 경우와 onclick 속성에서 링크 추출
        titles = soup.find_all('a', href=True)
        # print(titles)
        attr = ""

        for title in titles:

            title_text = title.get_text(strip=True)
            attr = title.get('href', '')

            # a 태그의 텍스트에 "pdf"이 포함된 경우
            if ".pdf" in attr:
                pdf_url = attr

    else:
        print(f"페이지를 불러오지 못했습니다. 상태 코드: {response.status_code}")

    # 필터링된 링크 배열 반환
    return pdf_url

# ----------------------------------------------------
# 상세 페이지 URL 리스트 생성
# ----------------------------------------------------
def get_link_list(url, printflag):
    # HTTP 요청을 보내고 응답 받기
    response = requests.get(url)
    response.raise_for_status()  # 상태 코드가 200이 아니면 예외 발생

    if printflag:
        print(response.text)

    # 결과를 저장할 배열
    links = []

    # 응답의 상태 코드 확인 (200번대면 정상)
    if response.status_code == 200:
        # HTML 파싱
        soup = BeautifulSoup(response.text, 'html.parser')
        # print(soup.text)

        # a 태그의 텍스트가 "주간기술동향"을 포함한 경우와 onclick 속성에서 링크 추출
        titles = soup.find_all('a', href=True)

        for title in titles:
            title_text = title.get_text(strip=True)
            onclick_attr = title.get('onclick', '')
            # print(title_text)

            # a 태그의 텍스트에 "주간기술동향"이 포함된 경우
            if "주간기술동향" in title_text and "periodicalViewA.it?" in onclick_attr:
                # print(title_text)
                # print(onclick_attr)
                extracted_url = "https://www.iitp.kr/kr/1/knowledge" + onclick_attr.replace("post_to_url( \".",
                                                                                            "").replace("\",\"post\");",
                                                                                                        "")
                # 발행일 추출
                # a 태그의 상위 td 태그의 다음 td에서 발행일 추출
                parent_td = title.find_parent('td')
                if parent_td:
                    date_match = re.search(r"\d{4}-\d{2}-\d{2}", parent_td.get_text(strip=True))
                    if date_match:
                        date_text = date_match.group()
                        try:
                            publish_date = datetime.datetime.strptime(date_text, "%Y-%m-%d").strftime("%Y%m%d")
                        except ValueError:
                            publish_date = '날짜 없음'
                    else:
                        publish_date = '날짜 없음'
                else:
                    publish_date = '날짜 없음'

                # 링크와 날짜를 배열로 추가
                links.append([extracted_url, publish_date])

    else:
        print(f"페이지를 불러오지 못했습니다. 상태 코드: {response.status_code}")

    # 필터링된 링크 배열 반환
    return links

# 주간기술동향
#url = "https://www.iitp.kr/kr/1/knowledge/periodicalListA.it?masterCode=publication&searClassCode=B_ITA_01"
url = "https://www.iitp.kr/kr/1/knowledge/periodicalListA.it?page=2&pageSize=20&searClassCode=B_ITA_01&currentPage=3"
# 각 페이지
# https://www.iitp.kr/kr/1/knowledge/periodicalViewA.it?searClassCode=B_ITA_01&masterCode=publication&identifier=1335
links = get_link_list(url, False)
# print(links)
print("START!!! #################################################################")

pdf_data_list = []
# 결과 출력
for i, link in enumerate(links):
    pdf_url = get_pdf_url(link[0], False)
    #save_file_name = "./PDF_Files/"+str(i)+".pdf"
    #save_file_name = "./PDF_Files/" + link[1] + ".pdf"
    save_file_name = "/content/drive/MyDrive/Text Summariza/data/01.BEFORE/" + link[1] + ".pdf"
    pdf_data_list.append(extract_text_from_pdf_url(pdf_url, save_file_name))
    # print(pdf_url)
print("File Count:"+str(i))
print("END!!! #################################################################")
# print(pdf_data_list[0])

# Commented out IPython magic to ensure Python compatibility.
#필요한 라이브러리 임포트
# %pip install llama-index
# %pip install utils
# %pip install llama-index-embeddings-openai
# %pip install llama-index-llms-openai

from google.colab import userdata
import os

os.environ["OPENAI_API_KEY"] = userdata.get('study')

#이미 실행 중인 이벤트 루프가 있는 환경에서 asyncio를 중첩하여 사용할 수 있도록 함
#특히 Jupyter 노트북과 같이 이벤트 루프가 이미 실행 중인 환경에서 비동기 코드를 실행할 때 유용
import nest_asyncio

nest_asyncio.apply()

import os
import pandas as pd
from llama_index.core import SimpleDirectoryReader
from llama_index.core.node_parser import SentenceSplitter
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import SummaryIndex

# 폴더 안의 모든 문서를 순차적으로 요약하는 함수
def summarize_documents_in_folder(folder_path: str, output_csv: str, file_count: int = None, llm=None, embed_model=None):
    # LLM (Large Language Model) 설정 (기본값 : OpenAI의 gpt-3.5-turbo 모델)
    llm = llm or OpenAI(model="gpt-3.5-turbo")

    # 임베딩 모델 설정 (기본값: OpenAI의 text-embedding-ada-002 모델)
    embed_model = embed_model or OpenAIEmbedding(model="text-embedding-ada-002")

    # 요약 결과를 저장할 리스트 (각 문서의 순번, 파일 경로, 원문, 요약문)
    summary_data = []

    # 폴더 내 모든 파일 경로 가져오기
    file_paths = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, file_name))]

    # file_count가 지정되지 않은 경우 모든 파일 사용
    if file_count is not None:
        file_paths = file_paths[:file_count]

    # 각 파일에 대해 요약 수행
    for index, file_path in enumerate(file_paths, start=1):  # 순번을 가져오기 위해 enumerate 사용, start=1로 설정해 1부터 시작
        # 문서 로드
        documents = SimpleDirectoryReader(input_files=[file_path]).load_data()

        # 문서 청크 분할
        splitter = SentenceSplitter(chunk_size=1024)
        nodes = splitter.get_nodes_from_documents(documents)

        # 요약 인덱스 생성
        summary_index = SummaryIndex(nodes)

        # 요약 쿼리 엔진 생성
        summary_query_engine = summary_index.as_query_engine(
            response_mode="tree_summarize",
            use_async=True,
            llm=llm
        )

        # 문서 요약 실행
        # summary = summary_query_engine.query("이 문서를 100자 이내로 문어체를 이용해서 한글로 요약")
        summary = summary_query_engine.query("이 문서를 한글로 요약")

        # 원문 텍스트 가져오기
        original_text = " ".join([node.get_content() for node in nodes])

        # 요약 결과를 리스트에 추가
        summary_data.append({
            "id": index,
            #"file_path": file_path,
            "dialogue": original_text,
            "summary": summary
        })

    # 데이터프레임 생성 및 CSV로 저장
    summary_df = pd.DataFrame(summary_data)
    summary_df.to_csv(output_csv, index=False, encoding='utf-8-sig')

    print(f"Summaries saved to {output_csv}")

# 사용 예시
folder_path = "/content/drive/MyDrive/Text Summariza/data/03.AFTER"  # 요약할 문서들이 있는 폴더 경로
output_csv = "/content/drive/MyDrive/Text Summariza/data/04.SUMMARY/summarized_documents.csv"  # 저장할 CSV 파일 경로
#summarize_documents_in_folder(folder_path, output_csv, 1)
summarize_documents_in_folder(folder_path, output_csv)

!pip install pymupdf

import fitz  # PyMuPDF 라이브러리
from datetime import datetime
import os
import re

def pdf_to_text_with_titles(pdf_path, txt_path):
    # 현재 날짜를 파일 이름으로 설정
    # current_date = datetime.now().strftime("%Y%m%d")
    # pdf_path = f"/content/drive/MyDrive/Text Summariza/data/01.BEFORE/{current_date}.pdf"
    # txt_path = f"/content/drive/MyDrive/Text Summariza/data/01.BEFORE/{current_date}.txt"

    # PDF 파일 열기
    pdf_document = fitz.open(pdf_path)

    # 텍스트 파일 생성 (쓰기 모드)
    with open(txt_path, 'w', encoding='utf-8') as txt_file:
        title_text = ""        # 대제목을 저장할 변수
        chapter_num = ""       # Chapter 번호를 저장할 변수
        in_title_section = False  # 현재 대제목을 작성 중인지 여부

        for page_num in range(pdf_document.page_count):
            # 각 페이지의 텍스트 블록 추출
            page = pdf_document[page_num]
            text_instances = page.get_text("dict")["blocks"]

            for block in text_instances:
                if "lines" in block:
                    for line in block["lines"]:
                        for span in line["spans"]:
                            font_size = span["size"]
                            text = span["text"].strip()

                            # 폰트 크기가 19 이상인 텍스트는 대제목으로 간주하여 title_text에 누적
                            if font_size >= 19:
                                title_text += text.replace(" ", "") + " "  # 불필요한 공백 제거 후 결합
                                in_title_section = True  # 대제목 상태로 전환

                            # "Chapter" 키워드 발견 시 다음 텍스트가 번호일 가능성 처리
                            elif text == "Chapter":
                                #print(f"\n[디버깅] Chapter 앞 텍스트: {title_text.strip()} (폰트 크기: {font_size})")
                                if in_title_section:
                                    chapter_num = ""  # Chapter 번호 초기화

                            # 숫자 형태의 텍스트가 대제목 상태에서 첫 등장 시 이를 Chapter 번호로 저장
                            elif text.isdigit() and in_title_section and not chapter_num:
                                chapter_num = title_text[-3:]  # Chapter 번호 추출
                                #print(f"[디버깅] Chapter 번호: {chapter_num}")

                            # "Chapter 번호 대제목" 형식으로 텍스트 파일에 저장
                            elif in_title_section and title_text and chapter_num:
                                # 포맷: "Chapter 번호 대제목"
                                title_text = title_text[:-3]
                                txt_file.write(f"Chapter {chapter_num} {title_text.strip()}\n")
                                #print(f"Chapter {chapter_num} {title_text.strip()}")

                                # 변수 초기화
                                title_text = ""
                                chapter_num = ""
                                in_title_section = False  # 대제목 상태 종료

                            # 일반 텍스트는 그대로 파일에 저장
                            else:
                                txt_file.write(text + " ")
                        txt_file.write("\n")  # 각 줄 구분
            txt_file.write("\n\n")  # 각 페이지 구분

    pdf_document.close()
    print(f"PDF 텍스트와 정리된 대제목이 {txt_path}에 저장되었습니다.")

def process_pdf_to_text_with_titles(folder_path, output_folder, file_count=None):
    # 폴더 안의 모든 파일 목록을 가져옵니다.
    files = os.listdir(folder_path)
    # PDF 파일만 필터링
    pdf_files = [f for f in files if f.lower().endswith('.pdf')]

    # 처리할 파일 개수가 지정된 경우 해당 개수만큼 파일을 선택합니다.
    if file_count is not None:
        pdf_files = pdf_files[:file_count]

    # PDF 파일을 순차적으로 처리합니다.
    for pdf_file in pdf_files:
        # PDF 파일 경로 생성
        pdf_path = os.path.join(folder_path, pdf_file)

        # 출력 텍스트 파일 경로 생성
        txt_file = os.path.basename(pdf_file).replace('.pdf', '.txt')
        txt_path = os.path.join(output_folder, txt_file)

        # pdf_to_text_with_titles 함수 호출
        pdf_to_text_with_titles(pdf_path, txt_path)
        print(f"Processed {pdf_file} to {txt_path}")

# 테스트: 1개 파일 처리
# pdf_folder = "/content/drive/MyDrive/Text Summariza/data/01.BEFORE"
# txt_folder = "/content/drive/MyDrive/Text Summariza/data/02.ING"
# process_pdf_to_text_with_titles(pdf_folder, txt_folder, file_count=1)

def split_and_save_chapters(txt_file_path, save_dir):
    # 저장 경로 설정
    #save_dir = "/content/drive/MyDrive/Text Summariza/data/02.AFTER/"
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    # txt 파일 읽기
    with open(txt_file_path, 'r', encoding='utf-8') as file:
        content = file.read()

    # txt_file_path에서 파일명 추출 (확장자 제거)
    base_name = os.path.splitext(os.path.basename(txt_file_path))[0]

    # Chapter 기준으로 분할
    chapters = content.split("Chapter ")[1:]  # "Chapter 0"을 제외한 나머지 분할

    for chapter in chapters:
        lines = chapter.splitlines()
        if len(lines) > 0:
            chapter_title_line = lines[0]
            # Chapter 번호와 제목 분리
            chapter_num = chapter_title_line.split()[0]
            if "0" not in str(chapter_num):
                continue
            chapter_title = ' '.join(chapter_title_line.split()[1:]).replace(" ", "")  # 제목의 공백 제거

            # 제목에서 파일명에 적합하지 않은 문자 및 슬래시 제거
            chapter_title = re.sub(r'[<>:"/\\|?*]', '', chapter_title)

            # 파일명으로 사용하기 위한 유효한 이름 형식으로 변환
            file_name = f"{base_name}_{chapter_num}_{chapter_title}.txt"
            file_path = os.path.join(save_dir, file_name)

            # 챕터 내용 추출
            chapter_content = "Chapter " + chapter.strip()  # 앞에 "Chapter " 추가

            # 챕터별 파일로 저장
            with open(file_path, 'w', encoding='utf-8') as chapter_file:
                chapter_file.write(chapter_content)

            print(f"{file_path} 저장 완료.")

# 테스트: 1개 파일 처리
# save_dir = "/content/drive/MyDrive/Text Summariza/data/03.AFTER"
# txt_file_path = "/content/drive/MyDrive/Text Summariza/data/02.ING/20240731.txt"
# split_and_save_chapters(txt_file_path, save_dir)

def process_split_and_save_chapters(input_folder, output_folder, file_count=None):
    # 폴더 안의 모든 파일 목록을 가져옵니다.
    files = os.listdir(input_folder)
    # TXT 파일만 필터링
    txt_files = [f for f in files if f.lower().endswith('.txt')]

    # 처리할 파일 개수가 지정된 경우 해당 개수만큼 파일을 선택합니다.
    if file_count is not None:
        txt_files = txt_files[:file_count]

    # TXT 파일을 순차적으로 처리합니다.
    for txt_file in txt_files:
        # TXT 파일 경로 생성
        txt_file_path = os.path.join(input_folder, txt_file)

        # pdf_to_text_with_titles 함수 호출
        split_and_save_chapters(txt_file_path, output_folder)
        print(f"Processed {txt_file_path} to {output_folder}")

# 테스트: 1개 파일 처리
#input_path = "/content/drive/MyDrive/Text Summariza/data/02.ING"
#output_path = "/content/drive/MyDrive/Text Summariza/data/03.AFTER"
#process_split_and_save_chapters(input_path, output_path, file_count=1)

print("START ================ process_pdf_to_text_with_titles ================")
pdf_folder = "/content/drive/MyDrive/Text Summariza/data/01.BEFORE"
txt_folder = "/content/drive/MyDrive/Text Summariza/data/02.ING"
#process_pdf_to_text_with_titles(pdf_folder, txt_folder, file_count=1)
process_pdf_to_text_with_titles(pdf_folder, txt_folder)
print("END!! ================ process_pdf_to_text_with_titles ================")

print("=======================================================================")
print("START ================ process_split_and_save_chapters ================")
input_path = "/content/drive/MyDrive/Text Summariza/data/02.ING"
output_path = "/content/drive/MyDrive/Text Summariza/data/03.AFTER"
#process_split_and_save_chapters(input_path, output_path, file_count=1)
process_split_and_save_chapters(input_path, output_path)
print("END!! ================ process_split_and_save_chapters ================")

# Mecab 설치를 위한 필수 라이브러리 설치
!apt-get update
!apt-get install -y g++ openjdk-8-jdk python3-dev python3-pip curl

!pip install mecab-python3

# 폰트 설치
!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf

!pip install sklearn
!apt-get install -y fonts-nanum

import os
os.kill(os.getpid(), 9)

!pip install konlpy

!apt-get update
!apt-get install -y mecab mecab-ipadic-utf8 mecab-ko mecab-ko-dic
!pip install mecab-python3

!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git

from google.colab import drive
drive.mount('/content/drive')

import os

# Mecab-ko-for-Google-Colab 리포지토리 클론할 경로 설정 (Google Drive 루트 디렉토리 아래)
repo_path = '/content/drive/Mecab-ko-for-Google-Colab'

# 이미 해당 경로에 리포지토리가 없다면 클론
if not os.path.exists(repo_path):
    !git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git "{repo_path}"
    print(f"Repository cloned to {repo_path}")
else:
    print(f"Repository already exists at {repo_path}")

cd Mecab-ko-for-Google-Colab

!bash install_mecab-ko_on_colab_light_220429.sh

# Commented out IPython magic to ensure Python compatibility.
import os
import sys
import warnings
import numpy as np
import pandas as pd
import json

import nltk
from konlpy.tag import Okt
import MeCab
import pandas as pd
import re
nltk.download('punkt')

from konlpy.tag import Mecab
mecab = Mecab()

# Okt 객체 생성
okt = Okt()
# MeCab Tokenizer 객체 생성
tokenizer = MeCab.Tagger()

import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
sns.set(style='white', context='notebook', palette='deep')

# 나눔 폰트 적용
import matplotlib.pyplot as plt
plt.rcParams['font.family'] = 'NanumBarunGothic'

from sklearn.feature_extraction.text import TfidfVectorizer
import itertools

if get_ipython():
    print("jupyter envirionment")
    PROJECT_DIR = os.path.abspath(os.path.join(os.path.dirname('__file__'), '..'))
    from tqdm import tqdm_notebook as tqdm  # 이거 안해주면 한 줄씩 출력됨 ;;
else:
    print("command shell envirionment")
    PROJECT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    from tqdm import tqdm

sys.path.insert(0, PROJECT_DIR)

import pandas as pd
import json

file_path = "/content/drive/MyDrive/Text Summariza/data/04.SUMMARY/summarized_documents.csv"

data = pd.read_csv(file_path)
data.head()

data = data.drop(columns=['id'])
data.sample(5)

len(data)

nltk.download('stopwords')
from bs4 import BeautifulSoup
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import warnings
warnings.filterwarnings("ignore", category=UserWarning, module='bs4')

# 영어 불용어
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

# Google Drive에 저장된 불용어 파일 경로
stopwords_file_path = '/content/drive/MyDrive/Text Summariza/stopword/stopword.txt'

with open(stopwords_file_path, 'r', encoding='utf-8') as f:
    final_stopwords = f.read().splitlines()

final_stopwords.extend(['chapter', '자료', '생성', '데이터 데이터', '기대', '효과', '작성', '수행', '자체', '작성', '연구', '그림', 'et al', '기업', '제공', '기술', '동향', '주간', '기획', 'www', 'iitp', '시리즈', '내용', '대해', '포함', '대한', '한다', '해야', '문제', '해결', 'conference', '발전', '2024', '정보', '통신', '다루', '발전', '응용', '분야', '역할', '중요', '활용', '향상'])
print(final_stopwords)

# 전체 문서 리스트 가져오기
documents = data['dialogue'].tolist() + data['summary'].tolist()

# TF-IDF 벡터라이저 설정 및 학습
vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words=final_stopwords)
tfidf_matrix = vectorizer.fit_transform(documents)

# 단어별 TF-IDF 스코어 저장
tfidf_scores = dict(zip(vectorizer.get_feature_names_out(), tfidf_matrix.sum(axis=0).tolist()[0]))

# MeCab Tokenizer 객체 생성
tokenizer_Mecab = MeCab.Tagger()

# 문장 분할 및 형태소 분석 후 불용어, 숫자 제거
def split_tokenize_and_remove_stopwords(text, threshold=0.2):

    # 문장 분할
    sentences = [sentence.strip() for sentence in re.split(r'[.!?]', text) if sentence]

    # 각 문장에 대해 형태소 분석 및 불용어 제거
    tokenized_sentences = []
    for sentence in sentences:
        parsed = tokenizer_Mecab.parse(sentence)  # 형태소 분석 수행

        # MeCab의 분석 결과 문자열을 형태소 단위로 나누기 (명사만 선택)
        morphs = [
            word.split('\t')[0]
            for word in parsed.splitlines()
            if word != 'EOS' and 'NN' in word.split('\t')[1].split(',')[0]  # 명사 품사만 선택 (NNG, NNP 등)
        ]

        # 불용어 제거
        #filtered_morphs = [morph for morph in morphs if morph not in final_stopwords]
        # 불용어 제거 및 TF-IDF 필터링
        filtered_morphs = [
            morph for morph in morphs
            if morph not in final_stopwords and
                morph.lower() not in ENGLISH_STOP_WORDS and  # 영어 불용어 제외
                # not re.match(r'\d+', morph)  # 숫자 제외 and
                tfidf_scores.get(morph, 0) >= threshold  # tfidf 임계치 이상
        ]
        tokenized_sentences.append(filtered_morphs)
    return tokenized_sentences

# 불용어 제거를 위한 전처리 예제
'''
import pandas as pd
from konlpy.tag import Okt
from nltk.tokenize import sent_tokenize

# 형태소 분석기
tokenizer_Okt = Okt()

def preprocess_text(text):
    # 형태소 분석 후 불용어 제거
    words = tokenizer_Okt.morphs(text)
    filtered_words = [word for word in words if word not in stopwords]
    return ' '.join(filtered_words)
'''

# 불용어 제거를 위한 전처리 예제
'''
import pandas as pd
from konlpy.tag import Okt
from nltk.tokenize import sent_tokenize

# 형태소 분석기
tokenizer_Okt = Okt()

def preprocess_text(text):
    # 형태소 분석 후 불용어 제거
    words = tokenizer_Okt.morphs(text)
    filtered_words = [word for word in words if word not in stopwords]
    return ' '.join(filtered_words)


# 문장 분할 및 형태소 분석 후 불용어, 숫자 제거
def split_tokenize_and_remove_stopwords(text, threshold=0.2):

    # 문장 분할
    sentences = [sentence.strip() for sentence in re.split(r'[.!?]', text) if sentence]

    # 각 문장에 대해 형태소 분석 및 불용어 제거
    tokenized_sentences = []
    for sentence in sentences:
        parsed = tokenizer.parse(sentence)  # 형태소 분석 수행

        # MeCab의 분석 결과 문자열을 형태소 단위로 나누기 (명사만 선택)
        morphs = [
            word.split('\t')[0]
            for word in parsed.splitlines()
            if word != 'EOS' and 'NN' in word.split('\t')[1].split(',')[0]  # 명사 품사만 선택 (NNG, NNP 등)
        ]

        # 불용어 제거
        #filtered_morphs = [morph for morph in morphs if morph not in final_stopwords]
        # 불용어 제거 및 TF-IDF 필터링
        filtered_morphs = [
            morph for morph in morphs
            if morph not in final_stopwords and
                morph.lower() not in ENGLISH_STOP_WORDS and  # 영어 불용어 제외
                # not re.match(r'\d+', morph)  # 숫자 제외 and
                tfidf_scores.get(morph, 0) >= threshold  # tfidf 임계치 이상
        ]
        tokenized_sentences.append(filtered_morphs)
    return tokenized_sentences
'''





"""# 텍스트 증강"""

from sklearn.model_selection import LeaveOneOut
import random

# 원본 데이터 불러오기
file_path = '/content/drive/MyDrive/Text Summariza/data/04.SUMMARY/summarized_documents.csv'
data = pd.read_csv(file_path)

# 데이터 개수 확인
print("원본 데이터 개수:", len(data))

# 랜덤 삭제 함수 정의
def random_deletion(text, p=0.3):
    """
    랜덤 삭제 증강 함수.
    입력된 텍스트에서 p 확률로 단어를 삭제하여 증강된 텍스트를 반환.
    """
    words = text.split()
    if len(words) == 1:
        return text
    new_words = [word for word in words if random.uniform(0, 1) > p]
    return ' '.join(new_words) if new_words else words[random.randint(0, len(words) - 1)]

# 증강 데이터 생성
augmented_texts = []
for text in data['dialogue']:  # 'dialogue' 열 사용
    augmented_texts.append(random_deletion(text))  # 랜덤 삭제 적용

# 원본 데이터와 증강 데이터를 하나의 데이터프레임으로 결합
data_augmented = pd.DataFrame({
    'dialogue': data['dialogue'].tolist() + augmented_texts,  # 원문과 증강된 텍스트 결합
    'summary': data['summary'].tolist() + data['summary'].tolist()  # 요약은 그대로 복사
})

# 증강된 데이터 개수 확인
print("증강 후 데이터 개수:", len(data_augmented))

data_augmented.head()

len(data_augmented)

data = data_augmented.copy()

import re
from bs4 import BeautifulSoup
from konlpy.tag import Okt
from string import punctuation

tokenizer = Okt()

# 데이터 전처리 함수
def preprocess_sentence(sentence, remove_stopwords=True):
    sentence = sentence.lower()
    sentence = BeautifulSoup(sentence, "lxml").text
    sentence = re.sub(r'\([^)]*\)', '', sentence)
    sentence = re.sub('"','', sentence)
    sentence = re.sub("[^가-힣a-zA-Z\s]", " ", sentence)

    # URL 제거
    pattern = '(http|ftp|https)://(?:[-\w.]|(?:%[\da-fA-F]{2}))+'
    sentence = re.sub(pattern=pattern, repl='', string=sentence)

    # 기타 특수 문자 제거
    sentence = re.sub('[-=+,#/\?:^$.@*\"※~&%ㆍ!』\\‘|\(\)\[\]\<\>`\'…》]', '', sentence)
    sentence = re.sub('\n', '.', sentence)

    words = tokenizer.morphs(sentence)

    # 불용어 제거
    if remove_stopwords:
        words = ' '.join(word for word in words if word not in final_stopwords and len(word) > 1)
    else:
        words = ' '.join(word for word in words if len(word) > 1)

    return words

clean_dialogue = []

for s in data['dialogue']:
    clean_dialogue.append(preprocess_sentence(s))

print("dialogue 전처리 후 결과: ", clean_dialogue[:1])

clean_summary = []

for s in data['summary']:
    clean_summary.append(preprocess_sentence(s, False))

print("summary 전처리 후 결과: ", clean_summary[:1])

data['dialogue_sentences'] = clean_dialogue
data['summary_sentences'] = clean_summary

data.replace('', np.nan, inplace=True)

#print(data['dialogue_sentences_NN'][0])
data.head()

# train_df에 적용하여 새로운 열 생성
data['dialogue_sentences_NN'] = data['dialogue'].apply(lambda row: split_tokenize_and_remove_stopwords(row))
data['summary_sentences_NN'] = data['summary'].apply(lambda row: split_tokenize_and_remove_stopwords(row))
data.head()
#print(train_df['dialogue_sentences'][0])

data.head()

"""# 시각화"""

# scikit-learn의 텍스트 벡터화 도구로, 텍스트 데이터를 수치 데이터로 변환
from sklearn.feature_extraction.text import CountVectorizer

def get_top_ngram(corpus, n=None, topn=20):
    #  n-그램: n개의 단어로 이루어진 조합
    # .fit(corpus) : corpus의 텍스트를 학습하여 각 n-그램의 단어를 vocabulary로 생성
    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)

    # .transform(corpus) : 해당 단어들을 Bag-of-Words 방식으로 변환
    # Bag-of-Words : 코퍼스에서 각 문장에 대해 해당 단어의 등장 횟수를 희소 행렬 형태로 나타냄 (단어의 등장 횟수)
    bag_of_words = vec.transform(corpus)

    # 각 단어의 전체 코퍼스에서의 등장 횟수를 합산
    # sum(axis=0)은 각 열(단어)마다의 모든 행(문장)에 걸쳐 열(단어)의 합계
    sum_words = bag_of_words.sum(axis=0)

    # vocabulary_ 속성을 사용하여 각 단어와 해당 인덱스를 가져옴
    # vec.vocabulary_ : CountVectorizer가 학습한 단어 사전으로, 각 단어와 그 인덱스를 맵핑하는 딕셔너리
    # 각 단어와 그 등장 횟수를 sum_words로부터 가져와서 리스트 형태로 저장
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]

    # sorted : 단어 빈도수를 기준으로 내림차순으로 정렬
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)

    # 상위 20개의 가장 빈도가 높은 n-그램을 반환
    return words_freq[:topn]

# Mecab : 한국어 형태소 분석기
#from konlpy.tag import Mecab
#tokenizer = Mecab()

import itertools

# train_df 데이터프레임에서 세 가지 열(추상적 요약, 추출적 요약, 비추출적 문장)로부터 바이그램을 추출하고
# 이를 시각화하여 각 데이터 열에서 자주 사용되는 바이그램(2-그램)의 빈도를 비교하는 작업을 수행

# Matplotlib의 subplot 2개 생성
fig,ax = plt.subplots(1,2,figsize=(15,15))

# --- abstractive -----------------------------------------
doc = data['dialogue_sentences_NN']
# 이미 형태소 분석된 문장 리스트를 사용하므로, 단어들을 공백으로 연결하여 텍스트로 변환
doc = doc.apply(lambda x: ' '.join(itertools.chain(*x)) if isinstance(x[0], list) else ' '.join(x))

# 상위 20개의 가장 빈도가 높은 바이(2)-그램 생성
top_n_bigrams = get_top_ngram(doc, 2, 20)

# top_n_bigrams의 단어와 빈도를 각각 x와 y 리스트로 분리
# top_n_bigrams : 바이그램과 그 빈도의 튜플로 구성된 리스트, [("word1 word2", count1), ("word3 word4", count2), ...]
# * : 언패킹 연산자, 시퀀스를 풀어서 각각의 요소를 인수로 전달
# zip(*top_n_bigrams) : 리스트의 각 요소를 언패킹하여 각각의 항목들을 묶어줌, ("word1 word2", "word3 word4", ...)와 (count1, count2, ...)
# map : zip의 결과로 얻어진 이터레이터들을 각각 리스트로 변환하여 반환
x, y = map(list, zip(*top_n_bigrams))

# sns.barplot: 상위 20개의 바이그램 빈도를 Seaborn의 바 차트로 시각화
# ax=ax[0] : 첫 번째 서브플롯에 그리도록 지정
# set_title : 해당 플롯의 제목을 설정
sns.barplot(x=y, y=x, ax=ax[0]).set_title('In dialogue_sentences')

# --- extractive -----------------------------------------
doc = data['summary_sentences_NN']
doc = doc.apply(lambda x: ' '.join(itertools.chain(*x)) if isinstance(x[0], list) else ' '.join(x))
top_n_bigrams = get_top_ngram(doc, 2, 20)
x, y = map(list, zip(*top_n_bigrams))
sns.barplot(x=y, y=x, ax=ax[1]).set_title('In extractive summary')

# Matplotlib의 전체 제목 설정
fig.suptitle("Common bigrams")

!pip install transformers  # Hugging Face의 NLP 모델 사용을 위한 라이브러리
!pip install datasets      # 데이터셋 로드 및 처리를 위한 라이브러리
!pip install evaluate      # 모델 성능 평가를 위한 다양한 메트릭 제공 라이브러리
!pip install rouge-score   # 텍스트 요약 평가를 위한 ROUGE 지표 계산 라이브러리
!pip install py7zr         # .7z 형식의 압축 파일을 처리하기 위한 라이브러리

# Transformers
from transformers import BartTokenizer, BartForConditionalGeneration
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments
from transformers import pipeline
from transformers import DataCollatorForSeq2Seq
import torch
import evaluate

from textblob import TextBlob
from sklearn.feature_extraction.text import TfidfVectorizer
import re
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

# Configuring Pandas to exhibit larger columns
'''
This is going to allow us to fully read the dialogues and their summary
'''
# pd.set_option('display.max_colwidth', 1000)
pd.set_option('display.max_colwidth', 20000)

seed = 42
colormap = 'cividis'
template = 'plotly_dark'

def display_feature_list(features, feature_type):

    print(f"\n{feature_type} Features: ")
    print(', '.join(features) if features else 'None')


def describe_df(df):

    global categorical_features, continuous_features, binary_features
    categorical_features = [col for col in df.columns if df[col].dtype == 'object']
    binary_features = [col for col in df.columns if df[col].nunique() <= 2 and df[col].dtype != 'object']
    continuous_features = [col for col in df.columns if df[col].dtype != 'object' and col not in binary_features]

    print(f"\n{type(df).__name__} shape: {df.shape}")
    print(f"\n{df.shape[0]:,.0f} samples")
    print(f"\n{df.shape[1]:,.0f} attributes")
    print(f'\nMissing Data: \n{df.isnull().sum()}')
    print(f'\nDuplicates: {df.duplicated().sum()}')
    print(f'\nData Types: \n{df.dtypes}')

    #negative_valued_features = [col for col in df.columns if (df[col] < 0).any()]
    #print(f'\nFeatures with Negative Values: {", ".join(negative_valued_features) if negative_valued_features else "None"}')

    display_feature_list(categorical_features, 'Categorical')
    display_feature_list(continuous_features, 'Continuous')
    display_feature_list(binary_features, 'Binary')

    print(f'\n{type(df).__name__} Head: \n')
    display(df.head(5))
    print(f'\n{type(df).__name__} Tail: \n')
    display(df.tail(5))


def histogram_boxplot(df, hist_color, box_color, height, width, legend, name):

    features = df.select_dtypes(include = [np.number]).columns.tolist()

    for feat in features:
        try:
            fig = make_subplots(
                rows=1,
                cols=2,
                subplot_titles=["Box Plot", "Histogram"],
                horizontal_spacing=0.2
            )

            density = gaussian_kde(df[feat])
            x_vals = np.linspace(min(df[feat]), max(df[feat]), 200)
            density_vals = density(x_vals)

            fig.add_trace(go.Scatter(x=x_vals, y = density_vals, mode = 'lines',
                                     fill = 'tozeroy', name="Density", line_color=hist_color), row=1, col=2)
            fig.add_trace(go.Box(y=df[feat], name="Box Plot", boxmean=True, line_color=box_color), row=1, col=1)

            fig.update_layout(title={'text': f'<b>{name} Word Count<br><sup><i>&nbsp;&nbsp;&nbsp;&nbsp;{feat}</i></sup></b>',
                                     'x': .025, 'xanchor': 'left'},
                             margin=dict(t=100),
                             showlegend=legend,
                             template = template,
                             #plot_bgcolor=bg_color,paper_bgcolor=paper_color,
                             height=height, width=width
                            )

            fig.update_yaxes(title_text=f"<b>Words</b>", row=1, col=1, showgrid=False)
            fig.update_xaxes(title_text="", row=1, col=1, showgrid=False)

            fig.update_yaxes(title_text="<b>Frequency</b>", row=1, col=2,showgrid=False)
            fig.update_xaxes(title_text=f"<b>Words</b>", row=1, col=2, showgrid=False)

            fig.show()
            print('\n')
        except Exception as e:
            print(f"An error occurred: {e}")

def plot_correlation(df, title, subtitle, height, width, font_size):

    corr = np.round(df.corr(numeric_only = True), 2)
    mask = np.triu(np.ones_like(corr, dtype = bool))
    c_mask = np.where(~mask, corr, 100)

    c = []
    for i in c_mask.tolist()[1:]:
        c.append([x for x in i if x != 100])



    fig = ff.create_annotated_heatmap(z=c[::-1],
                                      x=corr.index.tolist()[:-1],
                                      y=corr.columns.tolist()[1:][::-1],
                                      colorscale = colormap)

    fig.update_layout(title = {'text': f"<b>{title} Heatmap<br><sup>&nbsp;&nbsp;&nbsp;&nbsp;<i>{subtitle}</i></sup></b>",
                                'x': .025, 'xanchor': 'left', 'y': .95},
                    margin = dict(t=210, l = 110),
                    yaxis = dict(autorange = 'reversed', showgrid = False),
                    xaxis = dict(showgrid = False),
                    template = template,
                    #plot_bgcolor=bg_color,paper_bgcolor=paper_color,
                    height = height, width = width)


    fig.add_trace(go.Heatmap(z = c[::-1],
                             colorscale = colormap,
                             showscale = True,
                             visible = False))
    fig.data[1].visible = True

    for i in range(len(fig.layout.annotations)):
        fig.layout.annotations[i].font.size = font_size

    fig.show()

def compute_tfidf(df_column, ngram_range=(1,1), max_features=15):
    vectorizer = TfidfVectorizer(max_features=max_features, stop_words='english', ngram_range=ngram_range)
    x = vectorizer.fit_transform(df_column.fillna(''))
    df_tfidfvect = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names_out())
    return df_tfidfvect

describe_df(data)

!pip install plotly
!pip install numpy scipy

import plotly.graph_objects as go
from plotly.subplots import make_subplots
from scipy.stats import gaussian_kde

df_text_lenght = pd.DataFrame()
for feat in categorical_features:
    df_text_lenght[feat] = data[feat].apply(lambda x: len(str(x).split()))

histogram_boxplot(df_text_lenght, '#89CFF0', '#00509E', 600, 1000, True, 'Dataset')

from sklearn.model_selection import train_test_split

# 80% train, 20% temp
train, temp = train_test_split(data, test_size=0.2, random_state=42)

# temp = 10% validation, 10% test
val, test = train_test_split(temp, test_size=0.5, random_state=42)

# 각 데이터셋의 크기 확인
print("Train shape:", train.shape)
print("Validation shape:", val.shape)
print("Test shape:", test.shape)

"""# Modeling"""

!pip install datasets

from transformers import BartTokenizer, BartForConditionalGeneration
from datasets import Dataset

checkpoint = 'gogamza/kobart-summarization'

bart_tokenizer = BartTokenizer.from_pretrained(checkpoint)
model = BartForConditionalGeneration.from_pretrained(checkpoint)

print(model)

# 데이터 전처리 함수 정의
def preprocess_function(examples):
    inputs = [doc for doc in examples["dialogue_sentences"]]
    model_inputs = bart_tokenizer(inputs, max_length=1024, truncation=True, padding="max_length")

    with bart_tokenizer.as_target_tokenizer():
        labels = bart_tokenizer(examples["summary_sentences"], max_length=128, truncation=True, padding="max_length")

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Pandas DataFrame을 Dataset 형식으로 변환
train_ds = Dataset.from_pandas(train)
test_ds = Dataset.from_pandas(test)
val_ds = Dataset.from_pandas(val)

# 데이터셋 전처리 및 토크나이징
tokenized_train = train_ds.map(preprocess_function, batched=True,
                               remove_columns=['dialogue', 'summary', 'dialogue_sentences', 'summary_sentences', '__index_level_0__'])
tokenized_test = test_ds.map(preprocess_function, batched=True,
                              remove_columns=['dialogue', 'summary', 'dialogue_sentences', 'summary_sentences', '__index_level_0__'])
tokenized_val = val_ds.map(preprocess_function, batched=True,
                             remove_columns=['dialogue', 'summary', 'dialogue_sentences', 'summary_sentences', '__index_level_0__'])

from evaluate import load

metric = load('rouge')

def compute_metrics(eval_pred):
    predictions, labels = eval_pred

    # 예측값 디코딩
    decoded_preds = bart_tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # 실제 레이블 디코딩 (마스킹된 레이블 제거)
    labels = np.where(labels != -100, labels, bart_tokenizer.pad_token_id)
    decoded_labels = bart_tokenizer.batch_decode(labels, skip_special_tokens=True)

    # 한국어 문장 단위로 토큰화
    decoded_preds = ["\n".join(mecab.parse(pred.strip()).split()) for pred in decoded_preds]
    decoded_labels = ["\n".join(mecab.parse(label.strip()).split()) for label in decoded_labels]

    # ROUGE 점수 계산
    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=False)
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}  # 결과 추출

    # 생성된 문장의 평균 길이 추가
    prediction_lens = [np.count_nonzero(pred != bart_tokenizer.pad_token_id) for pred in predictions]
    result["gen_len"] = np.mean(prediction_lens)

    return {k: round(v, 4) for k, v in result.items()}

from transformers import PreTrainedTokenizerFast

tokenizer = PreTrainedTokenizerFast.from_pretrained("gogamza/kobart-base-v1")

from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq
from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast, pipeline

# Data Collator
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding='max_length', max_length=1024)

# Seq2Seq Training Arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="kobart_finetuned",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    seed=42,
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=2,
    weight_decay=0.01,
    save_total_limit=2,
    num_train_epochs=4,
    predict_with_generate=True,
    fp16=True,
    logging_dir='./logs',
    logging_strategy="steps",
    logging_steps=10,
    report_to="none"
)


trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=bart_tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

trainer.train()

import shutil

# 모델 평가
validation = trainer.evaluate(eval_dataset=tokenized_val)
print("검증 결과:")
print(validation)

# 학습된 모델 저장
directory = "kobart_finetuned"
trainer.save_model(directory)

tokenizer.save_pretrained(directory)

# 모델을 .zip 형식으로 저장
shutil.make_archive('kobart_finetuned', 'zip', directory)
shutil.move('kobart_finetuned.zip', './kobart_finetuned.zip')

# 저장된 모델 로드 및 요약 파이프라인
summarizer = pipeline('summarization', model=directory, tokenizer=bart_tokenizer)

from nltk.tokenize import sent_tokenize
import re

text = data['dialogue'][10] if 10 < len(data['dialogue']) else ""
reference_summary = data['summary'][10] if 10 < len(data['summary']) else ""

if not text or not reference_summary:
    print("데이터셋에서 대화 또는 요약이 누락되었습니다.")
else:
    paragraphs = text.split("\n\n")
    max_input_length = 1024

    generated_summaries = []
    for i, paragraph in enumerate(paragraphs):
        if len(paragraph.strip()) == 0:
            continue

        tokenized_paragraph = tokenizer(paragraph.strip())['input_ids']
        if len(tokenized_paragraph) > max_input_length:
            sentences = sent_tokenize(paragraph)
            chunks = [
                " ".join(sentences[i:i + 5]) for i in range(0, len(sentences), 5)
            ]
        else:
            chunks = [paragraph.strip()]

        for j, chunk in enumerate(chunks):
            print(f"\n요약 생성 중: 문단 {i + 1}, 청크 {j + 1}/{len(chunks)}")
            summary = summarizer(chunk, max_length=150, min_length=50, do_sample=False, num_beams=5)
            generated_summaries.append(summary[0]['summary_text'])

    combined_summary = " ".join(generated_summaries)
    combined_summary = re.sub(r'(\b\w+\b)\s+\1', r'\1', combined_summary)  # 중복 단어 제거
    combined_summary = ". ".join([sent.capitalize() for sent in combined_summary.split(". ")])  # 문장 정리

import textwrap

# 결과 출력 함수 정의
def print_wrapped(label, text, width=80):
    # print(f"\n{label} (길이: {len(text)} 문자):\n")
    print(f"\n{label}")
    print("\n".join(textwrap.wrap(text, width)))

# 결과 출력
print_wrapped("참조 요약", reference_summary)
print_wrapped("생성된 요약", combined_summary)